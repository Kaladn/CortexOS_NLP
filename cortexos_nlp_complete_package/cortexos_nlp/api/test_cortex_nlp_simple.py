"""
Simple test for CortexNLP class using direct processor import
"""

import sys
import os
import time

# Add project root to path
sys.path.append('/home/ubuntu/cortexos_nlp')

# Direct import of the integrated processor
from linguistic.integrated_processor import CortexLinguisticProcessor

class SimpleCortexNLP:
    """Simplified version of CortexNLP for testing"""
    
    def __init__(self):
        self._processor = CortexLinguisticProcessor()
        self._stats = {
            'documents_processed': 0,
            'total_tokens': 0,
            'total_processing_time': 0.0
        }
    
    def __call__(self, text):
        """Process text and return simplified results"""
        start_time = time.time()
        
        # Process through linguistic pipeline
        linguistic_doc = self._processor.process_text(text)
        
        # Update stats
        processing_time = time.time() - start_time
        self._stats['documents_processed'] += 1
        self._stats['total_tokens'] += len(linguistic_doc.tokens)
        self._stats['total_processing_time'] += processing_time
        
        # Return simplified doc-like object
        return SimpleDoc(text, linguistic_doc)
    
    def get_stats(self):
        """Get processing statistics"""
        stats = self._stats.copy()
        if stats['documents_processed'] > 0:
            stats['avg_processing_time'] = (
                stats['total_processing_time'] / stats['documents_processed']
            )
        return stats

class SimpleDoc:
    """Simplified Doc object for testing"""
    
    def __init__(self, text, linguistic_doc):
        self.text = text
        self._linguistic_doc = linguistic_doc
        self.tokens = [SimpleToken(token) for token in linguistic_doc.tokens]
    
    def __len__(self):
        return len(self.tokens)
    
    def __iter__(self):
        return iter(self.tokens)

class SimpleToken:
    """Simplified Token object for testing"""
    
    def __init__(self, linguistic_token):
        self.text = linguistic_token.text
        self.pos_ = getattr(linguistic_token, 'pos_tag', 'UNKNOWN')
        self.is_space = linguistic_token.is_space
        self.is_punct = linguistic_token.is_punct
        self.is_alpha = linguistic_token.is_alpha

def test_cortex_nlp_api():
    """Test the CortexNLP API functionality"""
    print("Testing CortexNLP API Layer")
    print("=" * 40)
    
    # Initialize
    cortex = SimpleCortexNLP()
    
    # Test sentences
    test_sentences = [
        "The quick brown fox jumps over the lazy dog.",
        "I can't believe it's working so well!",
        "Complex sentences with subordinate clauses are challenging."
    ]
    
    print("\n1. Testing single document processing:")
    for i, sentence in enumerate(test_sentences, 1):
        print(f"\n   {i}. Processing: '{sentence}'")
        
        start_time = time.time()
        doc = cortex(sentence)
        processing_time = time.time() - start_time
        
        print(f"      âœ“ Tokens: {len(doc)}")
        print(f"      âœ“ Processing time: {processing_time:.4f}s")
        print(f"      âœ“ POS tags: {[token.pos_ for token in doc if not token.is_space][:5]}...")
        
        # Validate basic functionality
        assert doc.text == sentence, "Text mismatch"
        assert len(doc) > 0, "No tokens generated"
        assert all(hasattr(token, 'text') for token in doc), "Invalid token objects"
    
    print("\n2. Testing API statistics:")
    stats = cortex.get_stats()
    
    print(f"   âœ“ Documents processed: {stats['documents_processed']}")
    print(f"   âœ“ Total tokens: {stats['total_tokens']}")
    print(f"   âœ“ Average processing time: {stats['avg_processing_time']:.4f}s")
    
    # Validate statistics
    assert stats['documents_processed'] == len(test_sentences), "Document count mismatch"
    assert stats['total_tokens'] > 0, "No tokens counted"
    assert stats['avg_processing_time'] > 0, "Invalid processing time"
    
    print("\n3. Testing token iteration:")
    doc = cortex("Hello world!")
    
    print(f"   Text: '{doc.text}'")
    print(f"   Tokens:")
    for i, token in enumerate(doc):
        print(f"     {i}: '{token.text}' (POS: {token.pos_}, Space: {token.is_space})")
    
    # Validate token properties
    content_tokens = [token for token in doc if not token.is_space]
    assert len(content_tokens) >= 2, "Not enough content tokens"
    assert all(token.text for token in content_tokens), "Empty token text"
    
    print("\n4. Testing determinism:")
    text = "The cat sits on the mat."
    
    # Process same text multiple times
    results = []
    for i in range(3):
        doc = cortex(text)
        token_data = [(token.text, token.pos_) for token in doc]
        results.append(token_data)
    
    # Check determinism
    for i, result in enumerate(results[1:], 2):
        assert result == results[0], f"Non-deterministic result in run {i}"
    
    print(f"   âœ“ Deterministic processing confirmed across 3 runs")
    
    print("\nâœ… CortexNLP API Layer Test: SUCCESS!")
    print("ğŸ‰ All API functionality working correctly!")
    print("ğŸ”§ Ready for full Doc/Token/Span implementation!")
    
    return True

if __name__ == "__main__":
    try:
        success = test_cortex_nlp_api()
        if success:
            print("\nğŸš€ API Layer foundation is solid!")
            print("ğŸ“‹ Next: Implement full Doc, Token, and Span classes")
    except Exception as e:
        print(f"\nâŒ Test failed: {str(e)}")
        import traceback
        traceback.print_exc()

